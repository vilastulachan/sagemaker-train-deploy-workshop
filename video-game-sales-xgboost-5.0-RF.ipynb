{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Product Success When Review Data Is Available\n",
    "## _**Using XGBoost to Predict Whether Sales will Exceed the \"Hit\" Threshold**_\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Train](#Train)\n",
    "1. [Real-time Inference](#Real-time)\n",
    "1. [Evaluation](#Evaluation)\n",
    "1. [Batch Inference](#Batch)\n",
    "1. [Clean Up](#CleanUp)\n",
    "\n",
    "---\n",
    "\n",
    "# Background\n",
    "\n",
    "Word of mouth in the form of user reviews, critic reviews, social media comments, etc. often can provide insights about whether a product ultimately will be a success. In the video game industry in particular, reviews and ratings can have a large impact on a game's success. However, not all games with bad reviews fail, and not all games with good reviews turn out to be hits. To predict hit games, machine learning algorithms potentially can take advantage of various relevant data attributes in addition to reviews.  \n",
    "\n",
    "For this notebook, we will work with the data set Video Game Sales with Ratings. This [Metacritic](http://www.metacritic.com/browse/games/release-date/available) data includes attributes for user reviews as well as critic reviews, sales, ESRB ratings, among others. Both user reviews and critic reviews are in the form of ratings scores, on a scale of 0 to 10 or 0 to 100. Although this is convenient, a significant issue with the data set is that it is relatively small.  \n",
    "\n",
    "Dealing with a small data set such as this one is a common problem in machine learning. This problem often is compounded by imbalances between the classes in the small data set. In such situations, using an ensemble learner can be a good choice.  This notebook will focus on using XGBoost, a popular ensemble learner, to build a classifier to determine whether a game will be a hit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup\n",
    "![Over of the workshop](img/overview2.png)\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- Import all the necessary libraries\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `get_execution_role()` call with the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import numpy as np                                \n",
    "import pandas as pd                               \n",
    "import matplotlib.pyplot as plt   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display                 \n",
    "from time import gmtime, strftime                 \n",
    "import sys                                        \n",
    "import math                                       \n",
    "import json\n",
    "import boto3\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need Sagemaker execution role\n",
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Copy the cleansed data\n",
    "- mkdir a s3 bucket\n",
    "- copy the files from the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bucket = \"s3://dc-summit-workshop-2021/sagemaker-train-deploy-model\"\n",
    "bucket_region = \"us-east-1\"\n",
    "\n",
    "my_bucket = \"2021-10-train-deploy-model-vilas2\"\n",
    "prefix = \"sagemaker/videogames_xgboost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket needs to be globally unise so use the following name and add your initial\n",
    "!aws s3 mb 's3://'$my_bucket --region us-east-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the cleansed data to my bucket\n",
    "!aws s3 cp $dataset_bucket 's3://'$my_bucket \\\n",
    "        --recursive --source-region us-east-1 --region us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Discuss and review the cleansed dataset\n",
    "\n",
    "Here's an initial look at the dataset and as you can see it needs cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The dataset has been cleaned and to help prevent overfitting the model, we'll randomly split the data into three groups. Specifically, the model will be trained on 70% of the data. It will then be evaluated on 20% of the data to give us an estimate of the accuracy we hope to have on \"new\" data. As a final testing dataset, the remaining 10% will be held out until the end.\n",
    "\n",
    "XGBoost operates on data in the libSVM data format, with features and the target variable provided as separate arguments. To avoid any misalignment issues due to random reordering, this split is done after the previous split in the above cell. As a last step before training, we'll copy the resulting files to S3 as input for SageMaker's managed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the review the cleansed dataset from s3 bucket\n",
    "model_df = pd.read_csv('s3://'+my_bucket+'/'+prefix+'/dataset/model_data.csv')\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the challenge in predicting \n",
    "plt.bar(['miss', 'hit'], model_df['HitorMiss'].value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train\n",
    "\n",
    "\n",
    "## We will use SageMaker built-in XGBoost algorithm.\n",
    "\n",
    "Our data is now ready to be used to train a XGBoost model. Need to define the algorithm, resource need for training along with location of test and validation data. Where to store the output. \n",
    "The XGBoost algorithm has many tunable hyperparameters. \n",
    "\n",
    "- `max_depth`: Maximum depth of a tree. As a cautionary note, a value too small could underfit the data, while increasing it will make the model more complex and thus more likely to overfit the data (in other words, the classic bias-variance tradeoff).\n",
    "- `eta`: Step size shrinkage used in updates to prevent overfitting.  \n",
    "- `eval_metric`: Evaluation metric(s) for validation data. For data sets such as this one with imbalanced classes, we'll use the AUC metric.\n",
    "- `scale_pos_weight`: Controls the balance of positive and negative weights, again useful for data sets having imbalanced classes.\n",
    "\n",
    "First we'll setup the parameters for a training job, then create a training job with those parameters and run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'videogames-xgboost-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job name = \", job_name)\n",
    "#print(\"my_bucket =\"+my_bucket)\n",
    "      \n",
    "\n",
    "containers = {\n",
    "                'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "                'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "                'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "                'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'\n",
    "             }\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"RoleArn\": role,\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": containers[boto3.Session().region_name],\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.c5.2xlarge\",\n",
    "        \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/train\".format(my_bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"CSV\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/validation\".format(my_bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"CSV\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/output\".format(my_bucket, prefix)\n",
    "    },\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"3\",\n",
    "        \"eta\":\"0.1\",\n",
    "        \"eval_metric\":\"auc\",\n",
    "        \"scale_pos_weight\":\"2.0\",\n",
    "        \"subsample\":\"0.5\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"100\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 60 * 60\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this takes around 4 min\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "# kick-off the training and pass the params\n",
    "sm.create_training_job(**create_training_params)\n",
    "\n",
    "# check when it's done\n",
    "status = sm.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(\"training job status = \"+status)\n",
    "\n",
    "try:\n",
    "    sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "finally:\n",
    "    status = sm.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(\"Training job ended with status, model saved: \" + status)\n",
    "    if status == 'Failed':\n",
    "        message = sm.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "        print('Training failed with the following error: {}'.format(message))\n",
    "        raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Amazon SageMaker inference options\n",
    "1. Synchronous Infererence\n",
    "1. Batch Inference\n",
    "\n",
    "\n",
    "## Real-time\n",
    "## 1. Amazon SageMaker Hosting Services\n",
    "Use Hosting Services when you require persistent endpoint - one prediction at a time for real time inference\n",
    "\n",
    "\n",
    "Now that we've trained the XGBoost algorithm on our data, let's prepare the model for hosting on a SageMaker serverless endpoint.  We will:\n",
    "\n",
    "1. Point to the scoring container\n",
    "1. Point to the model.tar.gz that came from training\n",
    "1. Create the hosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response = sm.create_model(\n",
    "    ModelName=job_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        'Image': containers[boto3.Session().region_name],\n",
    "        'ModelDataUrl': sm.describe_training_job(TrainingJobName=job_name)['ModelArtifacts']['S3ModelArtifacts']})\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we'll first configure - an endpoint configuration and then a HTTP endpoint\n",
    "\n",
    "1. EC2 instance type to use for hosting\n",
    "1. The initial number of instances\n",
    "1. Our hosting model name\n",
    "\n",
    "Let's configure the endpoint configuration first, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_endpoint_config = 'videogames-xgboost-endpoint-config-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "print(\" Endpoint configuration = \"+xgboost_endpoint_config)\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=xgboost_endpoint_config,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.t2.medium',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName': job_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "#print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll create the endpoint itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this can take around 6 mins\n",
    "\n",
    "xgboost_endpoint = 'videogames-xgb-endpoint-' + strftime(\"%Y%m%d%H%M\", gmtime())\n",
    "print(\"xgboost_endpoint = \"+xgboost_endpoint)\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=xgboost_endpoint,\n",
    "    EndpointConfigName=xgboost_endpoint_config)\n",
    "\n",
    "print(\" endpoint response = \"+create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=xgboost_endpoint)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint creation Status 1: \" + status)\n",
    "\n",
    "try:\n",
    "    sm.get_waiter('endpoint_in_service').wait(EndpointName=xgboost_endpoint)\n",
    "finally:\n",
    "    resp = sm.describe_endpoint(EndpointName=xgboost_endpoint)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Endpoint creation Status 2: \" + status)\n",
    "\n",
    "    if status != 'InService':\n",
    "        message = sm.describe_endpoint(EndpointName=xgboost_endpoint)['FailureReason']\n",
    "        print('Endpoint creation failed with the following error: {}'.format(message))\n",
    "        raise Exception('Endpoint creation did not succeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just extract the feature columns by dropping the label so we can use it for testing\n",
    "test_df = pd.read_csv('s3://'+my_bucket+'/'+prefix+'/test/test_data.csv')\n",
    "test_file0 = \"test_data.csv\"\n",
    "test_df.to_csv(test_file0, index=False)\n",
    "\n",
    "test_features_df = test_df.drop(test_df.columns[[0]], axis=1)\n",
    "#t upload and save the features csv file\n",
    "session = sagemaker.Session()\n",
    "test_file = \"test_features.csv\"\n",
    "test_features_df.to_csv(test_file, index=False)\n",
    "\n",
    "#test_s3_path = boto3.Session().resource('s3').Bucket(bucket).Object(prefix + '/data/test.csv').upload_file('test.csv')\n",
    "\n",
    "test_s3_path = session.upload_data(bucket=my_bucket, path=test_file, key_prefix=\"{}/data\".format(prefix))\n",
    "test_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just test in prediction\n",
    "#s3_client = boto3.client('s3')\n",
    "#s3_client.download_file(bucket, prefix+'/test/test_data.csv', './test_data.csv')\n",
    "\n",
    "#test_data = 's3://2021-10-train-deploy-xgboost-rf/sagemaker/videogames_xgboost/test/test_data.csv'\n",
    "with open('test_features.csv', 'r') as f:\n",
    "    for i in range(3,5):\n",
    "        single_test = f.readline()\n",
    "        response = runtime.invoke_endpoint(EndpointName = xgboost_endpoint,\n",
    "                                                  ContentType = 'text/csv',\n",
    "                                                  Body = single_test)\n",
    "        result = response['Body'].read().decode('ascii')\n",
    "        print('Predicted label is {}.'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "Now that we have our hosted endpoint, we can generate predictions from it. More specifically, let's generate predictions from our test data set to understand how well our model generalizes to data it has not seen yet.\n",
    "\n",
    "There are many ways to compare the performance of a machine learning model.  We'll start simply by comparing actual to predicted values of whether the game was a \"hit\" (`1`) or not (`0`).  Then we'll produce a  confusion matrix, which shows how many test data points were predicted by the model in each category versus how many test data points actually belonged in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation \n",
    "#model_df = pd.read_csv('s3://'+my_bucket+'/'+prefix+'/dataset/model_data.csv')\n",
    "batch_data = pd.read_csv('s3://'+my_bucket+'/'+prefix+'/data/test_hdr_data.csv')\n",
    "batch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the cto show output distribution\n",
    "plt.bar(['miss', 'hit'], batch_data['HitorMiss'].value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this needs to be replaced by csv file input\n",
    "def do_predict(data, endpoint_name, content_type):\n",
    "    payload = '\\n'.join(data)\n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType=content_type, \n",
    "                                   Body=payload)\n",
    "    result = response['Body'].read()\n",
    "    result = result.decode(\"utf-8\")\n",
    "    result = result.split(',')\n",
    "    #print(\"result = \"+str(result))\n",
    "    preds = [float((num)) for num in result]\n",
    "    preds = [round(num) for num in preds]\n",
    "    return preds\n",
    "\n",
    "# ----------------------------\n",
    "def bulk_predict(data, batch_size, endpoint_name, content_type):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "    \n",
    "    print(\" items = \"+str(items))\n",
    "    \n",
    "    for offset in range(0, items, batch_size):\n",
    "        if offset+batch_size < items:\n",
    "            results = do_predict(data[offset:(offset+batch_size)], endpoint_name, content_type)\n",
    "            #print(\" result = \"+str(results))\n",
    "            arrs.extend(results)\n",
    "        else:\n",
    "            arrs.extend(do_predict(data[offset:items], endpoint_name, content_type))\n",
    "        sys.stdout.write('.')\n",
    "    return(arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "\n",
    "with open('test_data.csv', 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "    #print(\" payload = \"+str(payload))\n",
    "    \n",
    "labels = [int(line.split(',')[0]) for line in payload.split('\\n')]\n",
    "\n",
    "with open('test_features.csv', 'r') as f2:\n",
    "     payload2 = f2.read().strip()\n",
    "\n",
    "\n",
    "test_data = [line for line in payload2.split('\\n')]\n",
    "#test_data\n",
    "#test_data = test_df.drop(test_df.columns[[0]], axis=1)\n",
    "#test_data.shape\n",
    "#print(\"test_data = \"+str(test_data))\n",
    "# do the prediction\n",
    "preds = bulk_predict(test_data, 100, xgboost_endpoint, 'text/csv')\n",
    "#print(\"preds = \"+str(preds))\n",
    "print ('\\nerror rate=%f' % ( sum(1 for i in range(len(preds)) if preds[i]!=labels[i]) /float(len(preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {'y_Actual':    np.array(labels),\n",
    "        'y_Predicted': np.array(preds)\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "sn.heatmap(confusion_matrix, annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the all games in the test set that actually are \"hits\" by our metric, the model correctly identified 74, while the overall error rate is 13%.  The amount of false negatives versus true positives can be shifted substantially in favor of true positives by increasing the hyperparameter scale_pos_weight. Of course, this increase comes at the expense of reduced accuracy/increased error rate and more false positives. How to make this trade-off ultimately is a business decision based on the relative costs of false positives, false negatives, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Batch\n",
    "## 2. Batch Inference - Amazon SageMaker Batch Transform\n",
    "\n",
    "Use Batch Transform when you need prediction for an entire dataset - one after another\n",
    "\n",
    "For this, we need to specify:\n",
    "- Hardware specification (instance count and type).  Prediction is embarassingly parallel, so feel free to test this with multiple instances, but since our dataset is not enormous, we'll stick to one.\n",
    "- `strategy`: Which determines how records should be batched into each prediction request within the batch transform job.  'MultiRecord' may be faster, but some use cases may require 'SingleRecord'.\n",
    "- `output_path`: The S3 location for batch transform to be output.  Note, file(s) will be named with '.out' suffixed to the input file(s) names.  In our case this will be 'train.csv.out'.  Note that in this case, multiple batch transform runs will overwrite existing values unless this is updated appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another option need to know what algo is here\n",
    "from sagemaker.transformer import Transformer\n",
    "output_path = f\"s3://{my_bucket}/{prefix}/output/\"\n",
    "\n",
    "transformer = Transformer(model_name=job_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    strategy=\"MultiRecord\",\n",
    "    max_payload=6,\n",
    "    max_concurrent_transforms=1,\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "transformer.transform(test_s3_path, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our batch transform job has completed, let's take a look at the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp \"s3://{my_bucket}/{prefix}/output/test_features.csv.out\" ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ./test_features.csv.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CleanUp\n",
    "\n",
    "This XGBoost model is just the starting point for predicting whether a game will be a hit based on reviews and other features.  There are several possible avenues for improving the model's performance.  First, of course, would be to collect more data and, if possible, fill in the existing missing fields with actual information.  Another possibility is further hyperparameter tuning, with Amazon SageMaker's Hyperparameter Optimization service.  And, although ensemble learners often do well with imbalanced data sets, it could be worth exploring techniques for mitigating imbalances such as downsampling, synthetic data augmentation, and other approaches.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=xgboost_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
